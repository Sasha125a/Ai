from http.server import HTTPServer, BaseHTTPRequestHandler
import json
import re
import random
import os
import base64
from datetime import datetime
import mimetypes
import math
from collections import defaultdict, Counter
import sqlite3
import requests
from bs4 import BeautifulSoup
import urllib.parse
import nltk
from nltk.stem import SnowballStemmer
from nltk.tokenize import word_tokenize
import warnings
warnings.filterwarnings('ignore')

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ NLTK
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class SimpleTextSimilarity:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è TF-IDF –∏ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    
    def __init__(self):
        self.vocab = set()
        self.doc_freq = defaultdict(int)
        self.documents = []
        self.stemmer = SnowballStemmer("russian")
    
    def fit(self, documents):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö"""
        self.documents = documents
        self.vocab = set()
        
        # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å –∏ —á–∞—Å—Ç–æ—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        for doc in documents:
            words = self._preprocess_text(doc)
            self.vocab.update(words)
            for word in set(words):
                self.doc_freq[word] += 1
    
    def _preprocess_text(self, text):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        words = [self.stemmer.stem(word) for word in words if len(word) > 2]
        return words
    
    def tfidf_vector(self, text):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ TF-IDF –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        words = self._preprocess_text(text)
        word_count = Counter(words)
        total_words = len(words)
        
        vector = {}
        for word in self.vocab:
            if word in word_count:
                # TF (Term Frequency)
                tf = word_count[word] / total_words
                # IDF (Inverse Document Frequency)
                idf = math.log(len(self.documents) / (1 + self.doc_freq[word]))
                vector[word] = tf * idf
            else:
                vector[word] = 0.0
        
        return vector
    
    def cosine_similarity(self, vec1, vec2):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        dot_product = sum(vec1.get(word, 0) * vec2.get(word, 0) for word in self.vocab)
        norm1 = math.sqrt(sum(val ** 2 for val in vec1.values()))
        norm2 = math.sqrt(sum(val ** 2 for val in vec2.values()))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def find_most_similar(self, query, documents):
        """–ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        query_vec = self.tfidf_vector(query)
        similarities = []
        
        for i, doc in enumerate(documents):
            doc_vec = self.tfidf_vector(doc)
            similarity = self.cosine_similarity(query_vec, doc_vec)
            similarities.append((similarity, i))
        
        similarities.sort(reverse=True)
        return similarities[0] if similarities else (0.0, -1)

class SimpleClassifier:
    """–ü—Ä–æ—Å—Ç–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    
    def __init__(self):
        self.patterns = {
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', 'hello', 'hi', '–¥–æ–±—Ä—ã–π', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ'],
            'farewell': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', 'bye', '–ø—Ä–æ—â–∞–π', '–¥–æ –≤—Å—Ç—Ä–µ—á–∏'],
            'help': ['–ø–æ–º–æ—â—å', 'help', '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å', '—Ñ—É–Ω–∫—Ü–∏–∏'],
            'explanation': ['–æ–±—ä—è—Å–Ω–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏', '—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–æ–∑–Ω–∞—á–∞–µ—Ç'],
            'code_request': ['–∫–æ–¥', '–ø—Ä–∏–º–µ—Ä', '–Ω–∞–ø–∏—à–∏', '—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π', '–ø–æ–∫–∞–∂–∏ –∫–æ–¥'],
            'comparison': ['—Ä–∞–∑–Ω–∏—Ü–∞', '—Å—Ä–∞–≤–Ω–∏', '—á—Ç–æ –ª—É—á—à–µ', '–æ—Ç–ª–∏—á–∏–µ', '–æ—Ç–ª–∏—á–∏—è'],
            'problem': ['–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–ø–æ–º–æ–≥–∏ —Ä–µ—à–∏—Ç—å', '–∏—Å–ø—Ä–∞–≤–∏—Ç—å'],
            'opinion': ['–º–Ω–µ–Ω–∏–µ', '–¥—É–º–∞–µ—à—å', '—Å—á–∏—Ç–∞–µ—à—å', '—Ç–æ—á–∫–∞ –∑—Ä–µ–Ω–∏—è'],
            'learning_path': ['—Å —á–µ–≥–æ –Ω–∞—á–∞—Ç—å', '–∫–∞–∫ —É—á–∏—Ç—å', '–ø—É—Ç—å –æ–±—É—á–µ–Ω–∏—è', '–∏–∑—É—á–µ–Ω–∏–µ'],
            'feedback': ['–æ—Ç–ª–∏—á–Ω–æ', '–ø–ª–æ—Ö–æ', '—Å–ø–∞—Å–∏–±–æ', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ', '—Ö–æ—Ä–æ—à–æ'],
        }
    
    def predict(self, text):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ intent'–∞ —Ç–µ–∫—Å—Ç–∞"""
        text_lower = text.lower()
        scores = defaultdict(int)
        
        for intent, keywords in self.patterns.items():
            for keyword in keywords:
                if keyword in text_lower:
                    scores[intent] += 1
        
        if scores:
            best_intent = max(scores.items(), key=lambda x: x[1])
            return [best_intent[0]] if best_intent[1] > 0 else []
        
        return []

class WebSearch:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    
    def __init__(self):
        self.search_engines = [
            self._search_duckduckgo,
            self._search_google_suggest,
            self._search_wikipedia
        ]
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_internet(self, query, max_results=3):
        """–ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        print(f"üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {query}")
        
        all_results = []
        
        for search_func in self.search_engines:
            try:
                results = search_func(query, max_results)
                if results:
                    all_results.extend(results)
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ {search_func.__name__}")
                    break  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —É—Å–ø–µ—à–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ {search_func.__name__}: {e}")
                continue
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        unique_results = []
        seen_snippets = set()
        
        for result in all_results:
            snippet = result.get('snippet', '')[:100]
            if snippet not in seen_snippets:
                seen_snippets.add(snippet)
                unique_results.append(result)
        
        return unique_results[:max_results]
    
    def _search_duckduckgo(self, query, max_results=3):
        """–ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo Instant Answer API"""
        try:
            url = "https://api.duckduckgo.com/"
            params = {
                'q': query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            results = []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', '–û—Ç–≤–µ—Ç'),
                    'snippet': data.get('AbstractText'),
                    'source': 'DuckDuckGo',
                    'url': data.get('AbstractURL', '')
                })
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
            for topic in data.get('RelatedTopics', [])[:max_results]:
                if 'Text' in topic:
                    results.append({
                        'title': topic.get('FirstURL', '').split('/')[-1].replace('_', ' '),
                        'snippet': topic['Text'],
                        'source': 'DuckDuckGo',
                        'url': topic.get('FirstURL', '')
                    })
            
            return results
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ DuckDuckGo: {e}")
            return []
    
    def _search_google_suggest(self, query, max_results=3):
        """–ò—Å–ø–æ–ª—å–∑—É–µ–º Google Suggestions –∫–∞–∫ fallback"""
        try:
            url = "http://suggestqueries.google.com/complete/search"
            params = {
                'q': query,
                'client': 'firefox',
                'hl': 'ru'
            }
            
            response = self.session.get(url, params=params, timeout=10)
            suggestions = response.json()[1]
            
            results = []
            for suggestion in suggestions[:max_results]:
                results.append({
                    'title': '–ü–æ–¥—Å–∫–∞–∑–∫–∞ Google',
                    'snippet': f"–í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É: {suggestion}",
                    'source': 'Google Suggest',
                    'url': ''
                })
            
            return results
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Google Suggest: {e}")
            return []
    
    def _search_wikipedia(self, query, max_results=2):
        """–ü–æ–∏—Å–∫ –≤ Wikipedia"""
        try:
            # –ò—â–µ–º —Å—Ç–∞—Ç—å—é –≤ Wikipedia
            url = "https://ru.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'list': 'search',
                'srsearch': query,
                'format': 'json',
                'srlimit': max_results
            }
            
            response = self.session.get(url, params=params, timeout=10)
            data = response.json()
            
            results = []
            for item in data.get('query', {}).get('search', [])[:max_results]:
                # –ü–æ–ª—É—á–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏
                extract_params = {
                    'action': 'query',
                    'prop': 'extracts',
                    'exintro': '1',
                    'explaintext': '1',
                    'titles': item['title'],
                    'format': 'json'
                }
                
                extract_response = self.session.get(url, params=extract_params, timeout=10)
                extract_data = extract_response.json()
                
                pages = extract_data.get('query', {}).get('pages', {})
                for page_id, page_data in pages.items():
                    if page_id != '-1' and 'extract' in page_data:
                        snippet = page_data['extract'][:300] + '...' if len(page_data['extract']) > 300 else page_data['extract']
                        results.append({
                            'title': item['title'],
                            'snippet': snippet,
                            'source': 'Wikipedia',
                            'url': f"https://ru.wikipedia.org/wiki/{urllib.parse.quote(item['title'])}"
                        })
            
            return results
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Wikipedia: {e}")
            return []

class LearningAI:
    """–ö–ª–∞—Å—Å –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ò–ò"""
    
    def __init__(self):
        self.knowledge_base = defaultdict(list)
        self.user_feedback = []
        self.conversation_patterns = []
        self.model_version = "1.0"
        self.learning_rate = 0.1
        self.stemmer = SnowballStemmer("russian")
        self.web_search = WebSearch()
        self.classifier = SimpleClassifier()
        self.similarity_engine = SimpleTextSimilarity()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π
        self.init_knowledge_db()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π
        self.load_knowledge()
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.initial_training()
    
    def init_knowledge_db(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö SQLite –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π"""
        self.conn = sqlite3.connect('ai_knowledge.db', check_same_thread=False)
        cursor = self.conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS qa_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT NOT NULL,
                answer TEXT NOT NULL,
                intent TEXT,
                entities TEXT,
                confidence REAL DEFAULT 1.0,
                usage_count INTEGER DEFAULT 0,
                success_rate REAL DEFAULT 1.0,
                source TEXT DEFAULT 'manual',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS web_search_cache (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                query TEXT NOT NULL,
                title TEXT,
                snippet TEXT,
                source TEXT,
                url TEXT,
                intent TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        self.conn.commit()
    
    def load_knowledge(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ QA –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        cursor.execute('SELECT question, answer, intent, confidence, source FROM qa_patterns')
        questions = []
        for question, answer, intent, confidence, source in cursor.fetchall():
            self.knowledge_base[intent].append({
                'question': question,
                'answer': answer,
                'confidence': confidence,
                'source': source
            })
            questions.append(question)
        
        # –û–±—É—á–∞–µ–º similarity engine –Ω–∞ –≤–æ–ø—Ä–æ—Å–∞—Ö
        if questions:
            self.similarity_engine.fit(questions)
    
    def initial_training(self):
        """–ù–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        training_data = [
            # –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
            ("–∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å –≤ python", "–ò—Å–ø–æ–ª—å–∑—É–π –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ class: class MyClass:", "code_request", "python"),
            ("—á—Ç–æ —Ç–∞–∫–æ–µ —Ñ—É–Ω–∫—Ü–∏—è", "–§—É–Ω–∫—Ü–∏—è —ç—Ç–æ –±–ª–æ–∫ –∫–æ–¥–∞, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ø–æ–ª–Ω—è–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É", "explanation", "programming"),
            ("–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ü–∏–∫–ª for", "–¶–∏–∫–ª for –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "explanation", "programming"),
            ("—á—Ç–æ —Ç–∞–∫–æ–µ –æ–æ–ø", "–û–û–ü - –æ–±—ä–µ–∫—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, –ø–æ–¥—Ö–æ–¥ –∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "explanation", "programming"),
            
            # –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è
            ("–ø—Ä–∏–≤–µ—Ç", "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —è –º–æ–≥—É –ø–æ–º–æ—á—å —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º?", "greeting", ""),
            ("–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ", "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å –∫–æ–¥–æ–º.", "greeting", ""),
            
            # –ü—Ä–æ—â–∞–Ω–∏—è
            ("–ø–æ–∫–∞", "–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –í–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ—Å—å —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.", "farewell", ""),
            ("–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "–î–æ –≤—Å—Ç—Ä–µ—á–∏! –£–¥–∞—á–∏ –≤ –∫–æ–¥–∏–Ω–≥–µ!", "farewell", ""),
            
            # –ü–æ–º–æ—â—å
            ("—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å", "–Ø –ø–æ–º–æ–≥–∞—é —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞, –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π, —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º", "help", ""),
            ("–ø–æ–º–æ—â—å", "–Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏. –ú–æ–≥—É –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –æ–±—ä—è—Å–Ω—è—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –ø–æ–º–æ–≥–∞—Ç—å —Å –æ—à–∏–±–∫–∞–º–∏", "help", ""),
        ]
        
        cursor = self.conn.cursor()
        for question, answer, intent, entities in training_data:
            cursor.execute('''
                INSERT OR IGNORE INTO qa_patterns (question, answer, intent, entities, source)
                VALUES (?, ?, ?, ?, 'manual')
            ''', (question, answer, intent, entities))
        
        self.conn.commit()
        self.load_knowledge()
    
    def analyze_intent(self, message):
        """–ê–Ω–∞–ª–∏–∑ intent'–∞ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        return self.classifier.predict(message)
    
    def search_and_learn(self, user_message, intent, entities, min_confidence=0.3):
        """–ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        print(f"üîç –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –¥–ª—è: {user_message}")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_query = self._build_search_query(user_message, intent, entities)
        
        # –ò—â–µ–º –≤ –∫—ç—à–µ
        cached_result = self._get_cached_search(search_query, intent)
        if cached_result:
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞")
            return cached_result, "web_cache"
        
        # –ò—â–µ–º –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
        search_results = self.web_search.search_internet(search_query)
        
        if not search_results:
            print("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ")
            return None, "no_results"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        formatted_answer = self._format_web_results(search_results, user_message, intent)
        
        if formatted_answer:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
            self._save_web_knowledge(user_message, formatted_answer, intent, entities, search_results)
            print("‚úÖ –ù–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É")
            return formatted_answer, "web_search"
        
        return None, "format_failed"
    
    def _build_search_query(self, user_message, intent, entities):
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        query = user_message
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
        programming_keywords = ["–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ", "–∫–æ–¥", "python", "javascript", "java", "–ø—Ä–æ–≥—Ä–∞–º–º–∞"]
        
        if intent in ['explanation', 'code_request']:
            if not any(keyword in user_message.lower() for keyword in programming_keywords):
                query = f"{user_message} –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"
        
        return query
    
    def _get_cached_search(self, query, intent):
        """–ü–æ–∏—Å–∫ –≤ –∫—ç—à–µ –≤–µ–±-–ø–æ–∏—Å–∫–∞"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT snippet, title, source 
            FROM web_search_cache 
            WHERE query = ? AND intent = ?
            ORDER BY created_at DESC 
            LIMIT 1
        ''', (query, intent))
        
        result = cursor.fetchone()
        if result:
            snippet, title, source = result
            return f"üìö {title}\n\n{snippet}\n\nüîó –ò—Å—Ç–æ—á–Ω–∏–∫: {source}"
        
        return None
    
    def _format_web_results(self, search_results, original_question, intent):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ –æ—Ç–≤–µ—Ç"""
        if not search_results:
            return None
        
        best_result = search_results[0]
        title = best_result.get('title', '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞')
        snippet = best_result.get('snippet', '')
        source = best_result.get('source', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç')
        
        # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        cleaned_snippet = self._clean_web_snippet(snippet, original_question)
        
        response = f"üåê **{title}**\n\n"
        response += f"{cleaned_snippet}\n\n"
        response += f"üìö *–ò—Å—Ç–æ—á–Ω–∏–∫: {source}*"
        
        return response
    
    def _clean_web_snippet(self, snippet, original_question):
        """–û—á–∏—Å—Ç–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –≤–µ–±-—Å–Ω–∏–ø–ø–µ—Ç–∞"""
        cleaned = re.sub(r'\s+', ' ', snippet).strip()
        cleaned = re.sub(r'<[^>]+>', '', cleaned)
        
        if len(cleaned) > 500:
            cleaned = cleaned[:500] + '...'
        
        return cleaned
    
    def _save_web_knowledge(self, question, answer, intent, entities, search_results):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–±-–∑–Ω–∞–Ω–∏–π –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        cursor = self.conn.cursor()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º QA –ø–∞—Ç—Ç–µ—Ä–Ω
        cursor.execute('''
            INSERT INTO qa_patterns (question, answer, intent, entities, source, confidence)
            VALUES (?, ?, ?, ?, 'web_search', 0.8)
        ''', (question, answer, intent, json.dumps(entities)))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –≤ –∫—ç—à
        for result in search_results[:2]:
            cursor.execute('''
                INSERT INTO web_search_cache (query, title, snippet, source, url, intent)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (question, result.get('title'), result.get('snippet'), 
                  result.get('source'), result.get('url'), intent))
        
        self.conn.commit()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –∑–Ω–∞–Ω–∏–π
        self.knowledge_base[intent].append({
            'question': question,
            'answer': answer,
            'confidence': 0.8,
            'source': 'web_search'
        })
    
    def find_best_response(self, user_message, intent, entities, use_web_search=True):
        """–ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
        cursor = self.conn.cursor()
        
        # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        cursor.execute('''
            SELECT answer, confidence, success_rate, source
            FROM qa_patterns 
            WHERE question = ? AND intent = ?
            ORDER BY success_rate DESC, confidence DESC
            LIMIT 1
        ''', (user_message, intent))
        
        result = cursor.fetchone()
        if result:
            answer, confidence, success_rate, source = result
            if confidence * success_rate > 0.5:
                cursor.execute('''
                    UPDATE qa_patterns 
                    SET usage_count = usage_count + 1 
                    WHERE question = ? AND answer = ?
                ''', (user_message, answer))
                self.conn.commit()
                return answer, confidence * success_rate, source
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏—Å–ø–æ–ª—å–∑—É—è –Ω–∞—à—É similarity engine
        cursor.execute('''
            SELECT question, answer, confidence, success_rate, source
            FROM qa_patterns 
            WHERE intent = ?
        ''', (intent,))
        
        all_questions = []
        qa_pairs = []
        
        for q, a, conf, success, source in cursor.fetchall():
            all_questions.append(q)
            qa_pairs.append((q, a, conf, success, source))
        
        if all_questions and qa_pairs:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à—É similarity engine –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
            similarity, index = self.similarity_engine.find_most_similar(user_message, all_questions)
            if similarity > 0.3 and index != -1:
                best_q, best_a, conf, success, source = qa_pairs[index]
                return best_a, similarity * conf * success, source
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –±–∞–∑–µ –∏ —Ä–∞–∑—Ä–µ—à–µ–Ω –≤–µ–±-–ø–æ–∏—Å–∫
        if use_web_search and intent in ['explanation', 'code_request', 'learning_path']:
            web_answer, web_source = self.search_and_learn(user_message, intent, entities)
            if web_answer:
                return web_answer, 0.7, web_source
        
        return None, 0.0, None

# –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏...
class SmartAI:
    def __init__(self):
        self.conversation_history = []
        self.user_profile = {
            'interests': set(),
            'skill_level': 'beginner',
            'preferred_languages': set(),
            'user_id': 'default'
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º
        self.learning_ai = LearningAI()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.learning_stats = {
            'conversations_processed': 0,
            'patterns_learned': 0,
            'web_searches': 0,
            'success_rate': 1.0
        }
        
        self.programming_languages = {
            'python': {'name': 'Python', 'paradigms': ['object-oriented', 'functional', 'imperative']},
            'javascript': {'name': 'JavaScript', 'paradigms': ['object-oriented', 'functional', 'event-driven']},
            'java': {'name': 'Java', 'paradigms': ['object-oriented', 'imperative']},
        }
    
    def analyze_intent(self, message):
        return self.learning_ai.analyze_intent(message)
    
    def extract_entities(self, message):
        entities = {
            'languages': [],
            'technologies': [],
            'concepts': [],
            'level_indicators': []
        }
        
        for lang_key, lang_info in self.programming_languages.items():
            lang_name = lang_info['name'].lower()
            if (lang_key in message.lower() or lang_name in message.lower()):
                entities['languages'].append(lang_key)
        
        return entities
    
    def generate_smart_response(self, message):
        # –ê–Ω–∞–ª–∏–∑ intent'–∞ –∏ entities
        intents = self.analyze_intent(message)
        entities = self.extract_entities(message)
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç (—Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        best_response = None
        best_confidence = 0.0
        response_source = "unknown"
        
        for intent in intents:
            response, confidence, source = self.learning_ai.find_best_response(
                message, intent, entities, use_web_search=True
            )
            if response and confidence > best_confidence:
                best_response = response
                best_confidence = confidence
                response_source = source
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç
        if best_response and best_confidence > 0.4:
            final_response = best_response
            if response_source == "web_search":
                self.learning_stats['web_searches'] += 1
                final_response = "üîç *–ù–∞—à–µ–ª –æ—Ç–≤–µ—Ç –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ:*\n\n" + final_response
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –æ—Ç–≤–µ—Ç
            final_response = self._craft_response(message, intents, entities)
            response_source = "generated"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            if response_source == "generated" and intents:
                primary_intent = intents[0]
                self.learning_stats['patterns_learned'] += 1
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.learning_stats['conversations_processed'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            'message': message,
            'response': final_response,
            'intents': intents,
            'entities': entities,
            'response_source': response_source,
            'confidence': best_confidence,
            'timestamp': datetime.now()
        })
        
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-25:]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        source_emoji = {
            "manual": "üìö",
            "web_search": "üåê", 
            "web_cache": "üíæ",
            "generated": "üéØ"
        }
        
        final_response += f"\n\n{source_emoji.get(response_source, 'ü§ñ')} *–ò—Å—Ç–æ—á–Ω–∏–∫: {response_source}*"
        
        return final_response
    
    def _craft_response(self, message, intents, entities):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ intent'–æ–≤"""
        if not intents:
            return "–ù–µ —Å–æ–≤—Å–µ–º –ø–æ–Ω—è–ª –≤–æ–ø—Ä–æ—Å. –ú–æ–∂–µ—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å? ü§î"
        
        if 'greeting' in intents:
            return self._generate_adaptive_greeting()
        
        if 'farewell' in intents:
            return self._generate_farewell()
        
        if 'help' in intents:
            return self._generate_help_response()
        
        if 'explanation' in intents:
            return self._generate_explanation(message, entities)
        
        if 'code_request' in intents:
            return self._generate_code_example(message, entities)
        
        if 'feedback' in intents:
            return self._process_feedback(message)
        
        return self._generate_contextual_response(message, entities)
    
    def _generate_adaptive_greeting(self):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ"""
        greetings = [
            "–ü—Ä–∏–≤–µ—Ç! –Ø —É–º–µ—é –∏—Å–∫–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –∏—Ö! üß†üåê",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –¢–µ–ø–µ—Ä—å —è –º–æ–≥—É –∏—Å–∫–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–Ω–ª–∞–π–Ω! üöÄ",
            "–ü—Ä–∏–≤–µ—Ç! –ú–æ—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ–ø–æ–ª–Ω—è–µ—Ç—Å—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞! üí´"
        ]
        return random.choice(greetings)
    
    def _generate_help_response(self):
        stats = self.get_learning_stats()
        
        help_text = f"""
ü§ñ **AI-GPT2 —Å –í–ï–ë-–ü–û–ò–°–ö–û–ú**

üß† **–ú–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
‚Ä¢ üîç **–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ** - –µ—Å–ª–∏ –Ω–µ –∑–Ω–∞—é –æ—Ç–≤–µ—Ç
‚Ä¢ üíæ **–ê–≤—Ç–æ-—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ** - –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∑–∞–ø–æ–º–∏–Ω–∞—é—Ç—Å—è
‚Ä¢ üåê **–†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏** - DuckDuckGo, Wikipedia
‚Ä¢ üìö **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π** –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {stats['total_conversations']}
‚Ä¢ –í–µ–±-–ø–æ–∏—Å–∫–æ–≤: {stats['web_searches']}
‚Ä¢ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {stats['knowledge_base_size']} –∑–∞–ø–∏—Å–µ–π

üí° **–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. –í—ã –∑–∞–¥–∞–µ—Ç–µ –≤–æ–ø—Ä–æ—Å
2. –Ø –∏—â—É –≤ —Å–≤–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
3. –ï—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–∂—É - –∏—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
4. –ù–∞–π–¥–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–æ—Ö—Ä–∞–Ω—è—é –≤ –±–∞–∑—É
5. –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –æ—Ç–≤–µ—á–∞—é –º–≥–Ω–æ–≤–µ–Ω–Ω–æ!
"""
        return help_text

    def get_learning_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        cursor = self.learning_ai.conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM qa_patterns')
        knowledge_size = cursor.fetchone()[0]
        
        stats = {
            'total_conversations': self.learning_stats['conversations_processed'],
            'patterns_learned': self.learning_stats['patterns_learned'],
            'web_searches': self.learning_stats['web_searches'],
            'knowledge_base_size': knowledge_size
        }
        
        return stats

    def _generate_code_example(self, message, entities):
        if entities['languages']:
            language = entities['languages'][0]
            examples = {
                'python': "```python\nprint('–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!')\n```",
                'javascript': "```javascript\nconsole.log('–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!');\n```", 
                'java': "```java\nSystem.out.println('–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!');\n```"
            }
            return f"–ü—Ä–∏–º–µ—Ä –Ω–∞ {language}:\n{examples.get(language, examples['python'])}"
        return "–ù–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ –Ω—É–∂–µ–Ω –ø—Ä–∏–º–µ—Ä –∫–æ–¥–∞?"

    def _generate_explanation(self, message, entities):
        return f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É '{message}' —è –ø–æ–∫–∞ –Ω–µ –Ω–∞—à–µ–ª —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ. –ü–æ–ø—Ä–æ–±—É—é –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ! üîç"

    def _process_feedback(self, message):
        return "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å! –ü—Ä–æ–¥–æ–ª–∂–∞—é —É—á–∏—Ç—å—Å—è –∏ —É–ª—É—á—à–∞—Ç—å –æ—Ç–≤–µ—Ç—ã! üìù"

    def _generate_contextual_response(self, message, entities):
        responses = [
            "–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤–æ–ø—Ä–æ—Å! –Ø –∑–∞–ø–æ–º–Ω—é –µ–≥–æ –¥–ª—è –±—É–¥—É—â–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.",
            "–£—á—É—Å—å –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ —Ç–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã! –°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-—Ç–æ –µ—â—ë.",
            "–ó–∞–ø–æ–º–Ω–∏–ª —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å! –°–æ –≤—Ä–µ–º–µ–Ω–µ–º –Ω–∞—É—á—É—Å—å –æ—Ç–≤–µ—á–∞—Ç—å –ª—É—á—à–µ."
        ]
        return random.choice(responses)

    def _generate_farewell(self):
        farewells = [
            "–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –í–æ–∑–≤—Ä–∞—â–∞–π—Ç–µ—Å—å —Å –Ω–æ–≤—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏! üëã",
            "–ü–æ–∫–∞! –£–¥–∞—á–∏ –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏! üöÄ",
            "–î–æ –≤—Å—Ç—Ä–µ—á–∏! –ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ, —è —É—á—É—Å—å –Ω–∞ –Ω–∞—à–∏—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–∞—Ö! üí´"
        ]
        return random.choice(farewells)

# –ö–ª–∞—Å—Å AIHandler –∏ –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏
class AIHandler(BaseHTTPRequestHandler):
    ai = SmartAI()
    
    def do_GET(self):
        if self.path == '/':
            self._serve_html()
        elif self.path == '/stats':
            self._serve_stats()
        else:
            self.send_error(404, "Not Found")
    
    def do_POST(self):
        if self.path == '/chat':
            self._handle_chat()
        else:
            self.send_error(404, "Not Found")
    
    def _serve_html(self):
        # HTML –∫–æ–¥ –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ –∫–∞–∫ –≤ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏
        html = '''
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>AI-GPT2 —Å –í–µ–±-–ü–æ–∏—Å–∫–æ–º üß†üåê</title>
            <style>
                body { 
                    font-family: -apple-system, BlinkMacSystemFont, sans-serif; 
                    max-width: 800px; 
                    margin: 0 auto; 
                    padding: 20px; 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                    min-height: 100vh;
                }
                .chat-container {
                    background: white;
                    border-radius: 20px;
                    padding: 25px;
                    box-shadow: 0 10px 30px rgba(0,0,0,0.3);
                    height: 90vh;
                    display: flex;
                    flex-direction: column;
                }
                .header {
                    text-align: center;
                    margin-bottom: 20px;
                    background: linear-gradient(135deg, #2c3e50, #3498db);
                    color: white;
                    padding: 20px;
                    border-radius: 15px;
                    margin: -25px -25px 20px -25px;
                }
                .learning-stats {
                    background: #e8f4fd;
                    padding: 15px;
                    border-radius: 10px;
                    margin: 10px 0;
                    border-left: 4px solid #3498db;
                }
                .stat-item {
                    display: flex;
                    justify-content: space-between;
                    margin: 5px 0;
                }
                .web-search-indicator {
                    background: #ffeb3b;
                    padding: 5px 10px;
                    border-radius: 15px;
                    font-size: 0.8em;
                    margin-left: 10px;
                    animation: pulse 2s infinite;
                }
                @keyframes pulse {
                    0% { opacity: 1; }
                    50% { opacity: 0.7; }
                    100% { opacity: 1; }
                }
                #chat {
                    flex: 1;
                    border: 2px solid #e1e5e9;
                    border-radius: 15px;
                    padding: 20px;
                    margin-bottom: 20px;
                    overflow-y: auto;
                    background: #f8f9fa;
                }
                .message {
                    margin: 15px 0;
                    padding: 15px 20px;
                    border-radius: 18px;
                    max-width: 85%;
                    line-height: 1.5;
                }
                .user {
                    background: linear-gradient(135deg, #007bff, #0056b3);
                    color: white;
                    margin-left: auto;
                    text-align: right;
                }
                .ai {
                    background: white;
                    color: #2c3e50;
                    border: 2px solid #3498db;
                }
            </style>
        </head>
        <body>
            <div class="chat-container">
                <div class="header">
                    <h1>üß†üåê AI-GPT2 —Å –í–ï–ë-–ü–û–ò–°–ö–û–ú</h1>
                    <p>–ò–ò –∫–æ—Ç–æ—Ä—ã–π –∏—â–µ—Ç –æ—Ç–≤–µ—Ç—ã –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∏—Ö!</p>
                    <div class="web-search-indicator">üîç –ê–ö–¢–ò–í–ï–ù –í–ï–ë-–ü–û–ò–°–ö</div>
                </div>
                
                <div class="learning-stats">
                    <h3>üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h3>
                    <div class="stat-item">
                        <span>–î–∏–∞–ª–æ–≥–æ–≤:</span>
                        <span id="conversationsCount">0</span>
                    </div>
                    <div class="stat-item">
                        <span>–í–µ–±-–ø–æ–∏—Å–∫–æ–≤:</span>
                        <span id="webSearches">0</span>
                    </div>
                    <div class="stat-item">
                        <span>–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π:</span>
                        <span id="knowledgeBase">0 –∑–∞–ø–∏—Å–µ–π</span>
                    </div>
                </div>
                
                <div id="chat">
                    <div class="message ai">
                        <strong>üß†üåê –ü—Ä–∏–≤–µ—Ç! –Ø AI-GPT2 —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º!</strong><br><br>
                        üí° <strong>–ú–æ–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:</strong><br>
                        ‚Ä¢ üîç <strong>–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫</strong> –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ<br>
                        ‚Ä¢ üíæ <strong>–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤</strong><br>
                        ‚Ä¢ üöÄ <strong>–ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã</strong> –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞<br><br>
                        üéØ <strong>–°–ø—Ä–æ—Å–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å - —è –Ω–∞–π–¥—É –æ—Ç–≤–µ—Ç!</strong>
                    </div>
                </div>
                
                <div style="display: flex; gap: 10px;">
                    <input type="text" id="messageInput" placeholder="–°–ø—Ä–æ—Å–∏—Ç–µ –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏..." style="flex: 1; padding: 15px; border: 2px solid #bdc3c7; border-radius: 25px;">
                    <button onclick="sendMessage()" style="padding: 15px 25px; background: #e74c3c; color: white; border: none; border-radius: 25px;">–û—Ç–ø—Ä–∞–≤–∏—Ç—å</button>
                </div>
            </div>

            <script>
                let stats = {
                    conversations: 0,
                    webSearches: 0
                };
                
                function addMessage(text, isUser) {
                    const chat = document.getElementById('chat');
                    const message = document.createElement('div');
                    message.className = isUser ? 'message user' : 'message ai';
                    message.innerHTML = text;
                    chat.appendChild(message);
                    chat.scrollTop = chat.scrollHeight;
                    
                    if (!isUser) {
                        stats.conversations++;
                        if (text.includes('üåê') || text.includes('–≤–µ–±-–ø–æ–∏—Å–∫')) {
                            stats.webSearches++;
                        }
                        updateStats();
                    }
                }

                async function sendMessage() {
                    const input = document.getElementById('messageInput');
                    const message = input.value.trim();
                    
                    if (!message) return;
                    
                    input.value = '';
                    addMessage(message, true);
                    
                    try {
                        const response = await fetch('/chat', {
                            method: 'POST',
                            headers: {'Content-Type': 'application/json'},
                            body: JSON.stringify({message: message})
                        });
                        
                        const data = await response.json();
                        addMessage(data.response, false);
                        
                    } catch (error) {
                        addMessage('‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è', false);
                    }
                }

                async function updateStats() {
                    try {
                        const response = await fetch('/stats');
                        const data = await response.json();
                        
                        document.getElementById('conversationsCount').textContent = data.total_conversations;
                        document.getElementById('webSearches').textContent = data.web_searches;
                        document.getElementById('knowledgeBase').textContent = data.knowledge_base_size + ' –∑–∞–ø–∏—Å–µ–π';
                    } catch (error) {
                        document.getElementById('conversationsCount').textContent = stats.conversations;
                        document.getElementById('webSearches').textContent = stats.webSearches;
                        document.getElementById('knowledgeBase').textContent = Math.floor(stats.conversations * 1.5) + ' –∑–∞–ø–∏—Å–µ–π';
                    }
                }

                document.getElementById('messageInput').addEventListener('keypress', function(e) {
                    if (e.key === 'Enter') sendMessage();
                });

                updateStats();
            </script>
        </body>
        </html>
        '''
        self.wfile.write(html.encode('utf-8'))
    
    def _serve_stats(self):
        """–û—Ç–¥–∞—á–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        stats = self.ai.get_learning_stats()
        
        self.send_response(200)
        self.send_header('Content-type', 'application/json')
        self.end_headers()
        
        self.wfile.write(json.dumps(stats).encode('utf-8'))
    
    def _handle_chat(self):
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))
            message = data.get('message', '')
            
            response = self.ai.generate_smart_response(message)
            
            self.send_response(200)
            self.send_header('Content-type', 'application/json; charset=utf-8')
            self.end_headers()
            
            self.wfile.write(json.dumps({"response": response}).encode('utf-8'))
            
        except Exception as e:
            self.send_error(500, f"Error: {str(e)}")

if __name__ == '__main__':
    HOST = '0.0.0.0'
    PORT = 8000
    
    print("üß†üåê –ó–ê–ü–£–°–ö AI-GPT2 –° –í–ï–ë-–ü–û–ò–°–ö–û–ú...")
    print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
    print("‚ïë           AI-GPT2 —Å –í–µ–±-–ü–æ–∏—Å–∫–æ–º v2.0        ‚ïë")
    print("‚ïë      –°–∞–º–æ–æ–±—É—á–∞—é—â–∏–π—Å—è –ò–ò —Å –ø–æ–∏—Å–∫–æ–º –æ–Ω–ª–∞–π–Ω    ‚ïë")
    print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
    print(f"üìç –°–µ—Ä–≤–µ—Ä: http://localhost:{PORT}")
    print("\nüéØ –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:")
    print("‚Ä¢ üîç –ê–≤—Ç–æ–ø–æ–∏—Å–∫ –≤ DuckDuckGo, Wikipedia")
    print("‚Ä¢ üíæ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤") 
    print("‚Ä¢ üìö –ê–≤—Ç–æ-–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    print("‚Ä¢ üöÄ –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π ML –±–µ–∑ scikit-learn")
    
    try:
        server = HTTPServer((HOST, PORT), AIHandler)
        print(f"‚úÖ AI-GPT2 —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –Ω–∞ {HOST}:{PORT}")
        print("üí° –¢–µ–ø–µ—Ä—å –ò–ò –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ª—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã!")
        server.serve_forever()
    except KeyboardInterrupt:
        print("\nüõë AI-GPT2 –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è...")
